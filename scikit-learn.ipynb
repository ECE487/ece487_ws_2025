{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn \n",
    "\n",
    "**A note on this document**\n",
    "This document is known as a Jupyter notebook; it allows text and executable code to coexist in a very easy-to-read format. Blocks can contain text or executable code. For blocks containing code, press `Shift + Enter`, `Ctrl+Enter`, or click the arrow on the block to run the code. Earlier blocks of code need to be run for the later blocks of code to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn, often abbreviated as sklearn, is an open-source machine learning library for Python. It is a widely used and highly popular library for various machine learning tasks, including classification, regression, clustering, dimensionality reduction, model selection, and more. Scikit-learn is built on top of other popular Python libraries such as NumPy and SciPy and provides a consistent and user-friendly API for various machine learning algorithms and tools.\n",
    "\n",
    "Scikit-learn is a valuable tool for both beginners and experienced data scientists and machine learning practitioners, as it simplifies the implementation of various machine learning models and algorithms and encourages good practices in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "`Pandas` is a popular Python library for data manipulation and analysis. It provides easy-to-use data structures and functions for working with structured data, making it an essential tool for data scientists and analysts. Here's an introductory tutorial on how to get started with Pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frames\n",
    "\n",
    "`pandas.DataFrame` is the primary data structure in Pandas. You can create a DataFrame from various data sources like dictionaries, lists, CSV files, Excel files, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Mark\", \"Lisa\", \"Luke\"],\n",
    "    \"Age\": [25, 30, 35, 18, 45, 29],\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly inspect the data in our DataFrame using various methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info() provides a concise summary of the DataFrame.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape returns the number of rows and columns.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head() displays the first 5 rows.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.tail() shows the last 5 rows.\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access specific data in your DataFrame using various indexing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing a column: df['ColumnName'] or df.ColumnName\n",
    "df.Name  # or df[\"Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing rows by index: df.loc[row_index]\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series\n",
    "\n",
    "`pandas.Series` is a class and data structure that is part of the Pandas library in Python. It represents a one-dimensional labeled array that can hold data of various types, similar to a column in a spreadsheet or a single-dimensional array. \n",
    "\n",
    "In practice, a Pandas Series can be thought of as a combination of a **NumPy array** (providing efficient numerical operations) and a **Python dictionary** (providing label-based access). You can use the labels to access and manipulate the data, and you can perform various data analysis tasks like filtering, aggregation, and visualization with ease.\n",
    "\n",
    "Here's a quick tutorial on how to work with Pandas Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We can create a Pandas Series from various data sources, such as lists, NumPy arrays, dictionaries, or even scalar values.\n",
    "data = [10, 20, 30, 40, 50]\n",
    "label = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "series = pd.Series(data, index=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access elements in a Series using indexing, just like with lists.\n",
    "print(series.iloc[1])  # Access data by position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can access elements in a Series using custom indexing\n",
    "print(series[\"B\"])  # Access data by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can perform mathematical operations on Series, and the operations are applied element-wise.\n",
    "print(series * 2)  # Perform arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can filter and slice a Series based on conditions or positions.\n",
    "print(series[series > 40])  # Filter elements greater than 40\n",
    "print(series[1:4])  # Slice from the second to the fourth element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Series support various operations like addition, subtraction, and more. You can also perform aggregation operations like sum, mean, and max.\n",
    "print(series.sum())\n",
    "print(series.mean())\n",
    "print(series.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with scikit-learn\n",
    "\n",
    "Let's revisit the linear regression problem we previously explored this semester, but this time we will employ the `scikit-learn` libraries. We will examine the following linear equation for our regression task: \n",
    "$$y = 1 + 2x + \\zeta$$\n",
    "In this equation, we have a weight vector $\\mathbf{w} = [1 \\quad 2]^\\top$, and $\\zeta$ represents Gaussian noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)  # Independent variable\n",
    "y = 2 * X + 1 + 0.3 * np.random.rand(100, 1)  # Dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` function in scikit-learn is a utility for splitting a dataset into two or more parts, typically for the purpose of creating a **training set** and a **testing (or validation) set**. This is a crucial step in machine learning to avoid data leakage and evaluate the performance of a model on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"The sample data size is {len(X)}\")\n",
    "print(f\"The training data size is {len(X_train)}\")\n",
    "print(f\"The testing data size is {len(X_test)}\")\n",
    "\n",
    "# Plot the training and testing data\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(X_train, y_train, label=\"Training Data\", color=\"blue\")\n",
    "plt.scatter(X_test, y_test, label=\"Test Data\", color=\"green\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.linear_model` is a module within scikit-learn, a popular machine learning library for Python. `The sklearn.linear_model` module provides classes and functions for linear modeling, including linear regression, logistic regression, and other linear-based models. \n",
    "\n",
    "The `fit` method in machine learning is used to train a machine learning model on a given dataset. It is one of the fundamental steps in the machine learning workflow where the model learns patterns and relationships in the data to make predictions or classifications. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a linear regression model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print the model's coefficients (w1, w2, ...) and intercept (w0)\n",
    "print(\"Coefficients:\", linear_reg.coef_)\n",
    "print(\"Intercept:\", linear_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the `LinearRegression` model in scikit-learn does not have the same hyperparameters related to _iterations_ or _learning rates_ as iterative optimization-based models like gradient descent. Scikit-learn's `LinearRegression` uses a _closed-form solution_, so there's no concept of adjusting the number of iterations or learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training data and the regression line\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(X_train, y_train, color=\"blue\", label=\"Training Data\")\n",
    "plt.plot(X, linear_reg.predict(X), color=\"red\", linewidth=1.5, label=\"Regression Line\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.predict` method is used to make predictions for the **test data** or infer target values based on input features. It takes an array-like input of feature values and returns predictions for the corresponding target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = linear_reg.predict(X_test)\n",
    "\n",
    "# Plot the testing data and the regression line\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(X_test, y_test, label=\"Test Data\", color=\"green\")\n",
    "plt.plot(X, linear_reg.predict(X), color=\"red\", linewidth=1.5, label=\"Regression Line\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R-squared value, also known as the coefficient of determination, is a statistical measure used to evaluate the goodness of fit of a regression model to a dataset. It quantifies the proportion of the variance in the dependent variable (target) that is explained by the independent variables (features) in the model. _**In other words, it assesses how well the regression model captures the variation in the data.**_\n",
    "\n",
    "The R-squared value, denoted as $R^2$ is calculated using the following mathematical equation:\n",
    "\n",
    "$$ R^2 = \\frac{SSR}{SST} = \\frac{SST - SSE}{SST}$$\n",
    "\n",
    "where $SSR$ is the sum of squares for regression, $SSE$ is the sum of squared errors, and $SST$ is the total sum of squares. The terms $SSR$, $SSE$ and $SST$ are defined as follows:\n",
    "\n",
    "$$ SSR = \\sum_{i=1}^{N}(\\hat{y}_i - \\bar{y})^2 $$\n",
    "\n",
    "$$ SSE = \\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "$$ SST = \\sum_{i=1}^{N}(y_i - \\bar{y})^2 $$\n",
    "\n",
    "The R-squared can be interpreted as the proportion of the total variation in the dependent variable that is explained by the regression model. _**It ranges from 0 (no explanatory power) to 1 (perfect explanation).**_ A higher R-squared value indicates a better fit of the model to the data, as it implies that a larger proportion of the variance is explained by the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's R-squared score\n",
    "r_squared = linear_reg.score(X_test, y_test)\n",
    "print(\"R-squared:\", r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a critical technique for evaluating the performance of machine learning models. `Scikit-learn` provides a convenient way to perform cross-validation using its `cross_val_score` function. This function helps you compute scores for a specific metric (e.g., R-squared) on different subsets of your dataset. Here's an example of how to perform cross-validation using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the cross-validation method. In this case, we use 5-fold cross-validation.\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=25)\n",
    "\n",
    "# Perform cross-validation and calculate the scores\n",
    "scores = cross_val_score(linear_reg, X, y, cv=kf, scoring=\"r2\")\n",
    "\n",
    "# Print the cross-validation scores and their mean\n",
    "print(\"Cross-validation R^2 scores:\", scores)\n",
    "print(\"Mean R^2:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 1\n",
    "\n",
    "Use the `scikit-learn` libraries for the `Falling Body` example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "y0 = 60\n",
    "v0 = 20\n",
    "g = -9.8067\n",
    "\n",
    "data = pd.read_csv(\"./data/falling_body.csv\")\n",
    "t = data[\"time\"]\n",
    "y = data[\"y\"]\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(t, y, \"o\")\n",
    "plt.xlabel(\"time, sec\")\n",
    "plt.ylabel(\"height (y), m\", rotation=90)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack([np.ones_like(t), t, t**2])\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Split the dataset into two parts; a training set and a testing (or validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "# print the sample data, training data, and testing data sizes.\n",
    "\n",
    "# Plot the training and testing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Train the model on the training data and print the optimal coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# Fit the model to the training data and find the optimal coefficients.\n",
    "# Create a linear regression model\n",
    "\n",
    "# Fit the model to the training data\n",
    "\n",
    "# Print the model's coefficients and intercept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO` Plot the testing data and the regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the testing data and the regression line\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Calculate the model's R-squared score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model's R-squared score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Use 5-fold cross-validation to find $R^2$ scores and their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 5-fold cross-validation to find $R^2$ scores and their average.\n",
    "# Define the cross-validation method. In this case, we use 5-fold cross-validation.\n",
    "# Perform cross-validation and calculate the scores\n",
    "# Print the cross-validation scores and their mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with scikit-learn\n",
    "\n",
    "Let's revisit the logistic regression binary classification problem where we want to predict whether a student will pass (1) or fail (0) an exam based on the number of hours they studied and the number of hours they slept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_exam_data():\n",
    "    # Specify the file name\n",
    "    file_name = \"./data/exam_data.csv\"\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "\n",
    "exam_df = read_exam_data()\n",
    "print(exam_df)\n",
    "\n",
    "# Separate the data into Passed (Y=1) and Failed (Y=0)\n",
    "passed = exam_df[exam_df[\"Passed Exam (Y)\"] == 1]\n",
    "failed = exam_df[exam_df[\"Passed Exam (Y)\"] == 0]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(\n",
    "    passed[\"Hours Studied (X1)\"],\n",
    "    passed[\"Hours Slept (X2)\"],\n",
    "    label=\"Passed\",\n",
    "    color=\"r\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    failed[\"Hours Studied (X1)\"],\n",
    "    failed[\"Hours Slept (X2)\"],\n",
    "    label=\"Failed\",\n",
    "    color=\"b\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "plt.xlabel(\"Hours Studied (X1)\")\n",
    "plt.ylabel(\"Hours Slept (X2)\")\n",
    "plt.title(\"Exam Data Scatter Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, we will use the `train_test_split` function in scikit-learn for splitting a dataset into two or more parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x1 = exam_df[\"Hours Studied (X1)\"]\n",
    "x2 = exam_df[\"Hours Slept (X2)\"]\n",
    "y = exam_df[\"Passed Exam (Y)\"]\n",
    "\n",
    "X = np.column_stack([x1, x2])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `fit` method for logistic regression to train a machine learning model on a given dataset. \n",
    "\n",
    "We can control the number of iterations and learning rate for logistic regression in scikit-learn. Scikit-learn's logistic regression implementation uses the 'liblinear' solver by default, which allows you to set the maximum number of iterations and the learning rate through the max_iter and C parameters, respectively.\n",
    "\n",
    "We can set the maximum number of iterations for the solver like this.\n",
    "\n",
    "```\n",
    "model = LogisticRegression(solver='liblinear', max_iter=100, C=1.0)  # default values\n",
    "```\n",
    "\n",
    "We will use the default settings without specifying any function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "\n",
    "# Create a logistic regression model\n",
    "logistic_reg = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logistic_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print the model's coefficients and intercept\n",
    "print(\"Coefficients:\", logistic_reg.coef_)\n",
    "print(\"Intercept:\", logistic_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict_proba` method is commonly used in machine learning, especially for binary and multiclass classification problems. The purpose of `predict_proba` is to estimate the probability of each class label for a given input, allowing you to make more informed decisions when dealing with classification tasks. The `predict_proba` method returns a probability distribution over the classes. In binary classification, it typically returns a 2D array with two columns, where each column represents the probability of the data point belonging to one of the two classes. The sum of these probabilities for each class will be equal to 1.0. In multiclass classification, the method returns the probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities for the positive class (class 1)\n",
    "probs = logistic_reg.predict_proba(X_test)\n",
    "\n",
    "for x, prob in zip(X_test, probs[:, 1]):\n",
    "    print(f\"study hours: {x[0]}, sleep hours: {x[1]}, probability to pass: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate the data into Passed (Y=1) and Failed (Y=0)\n",
    "threshold = 0.5\n",
    "testdata_passed = X_test[probs[:, 1] >= threshold]\n",
    "testdata_failed = X_test[probs[:, 1] < threshold]\n",
    "\n",
    "traindata_passed = X_train[logistic_reg.predict_proba(X_train)[:, 1] >= threshold]\n",
    "traindata_failed = X_train[logistic_reg.predict_proba(X_train)[:, 1] < threshold]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(\n",
    "    passed[\"Hours Studied (X1)\"],\n",
    "    passed[\"Hours Slept (X2)\"],\n",
    "    label=\"Passed\",\n",
    "    color=\"r\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    failed[\"Hours Studied (X1)\"],\n",
    "    failed[\"Hours Slept (X2)\"],\n",
    "    label=\"Failed\",\n",
    "    color=\"b\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Hours Studied (X1)\")\n",
    "plt.ylabel(\"Hours Slept (X2)\")\n",
    "plt.title(\"Exam Data\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "plt.scatter(\n",
    "    traindata_passed[:, 0],\n",
    "    traindata_passed[:, 1],\n",
    "    label=\"Test Data Passed\",\n",
    "    color=\"r\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    traindata_failed[:, 0],\n",
    "    traindata_failed[:, 1],\n",
    "    label=\"Test Data Failed\",\n",
    "    color=\"b\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    testdata_passed[:, 0],\n",
    "    testdata_passed[:, 1],\n",
    "    label=\"Test Data Passed\",\n",
    "    color=\"r\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    testdata_failed[:, 0],\n",
    "    testdata_failed[:, 1],\n",
    "    label=\"Test Data Failed\",\n",
    "    color=\"b\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Hours Studied (X1)\")\n",
    "plt.ylabel(\"Hours Slept (X2)\")\n",
    "plt.title(\"Train & Test Result\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the trained model to make predictions for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = logistic_reg.predict(X_test)\n",
    "\n",
    "# confusion matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# classification report\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "print(\"Classification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a resampling procedure used in machine learning and model evaluation to assess how well a predictive model will perform on an independent dataset. It's a critical step to ensure that a machine learning model's performance is not overly optimistic or pessimistic and to detect issues like overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross-validation method. In this case, we use 5-fold cross-validation.\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# Perform cross-validation and calculate the scores\n",
    "scores = cross_val_score(logistic_reg, X, y, cv=kf, scoring=\"r2\")\n",
    "\n",
    "# Print the cross-validation scores and their mean\n",
    "print(\"Cross-validation R^2 scores:\", scores)\n",
    "print(\"Mean R^2:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation commonly used to evaluate the performance of binary classification models, such as logistic regression, support vector machines, or decision trees. The ROC curve is a tool for visualizing the trade-off between a model's true positive rate (sensitivity) and its false positive rate (1-specificity) across different classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs[:, 1])\n",
    "\n",
    "# Calculate the AUC (Area Under the ROC Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 2\n",
    "\n",
    "The scikit-learn breast cancer dataset is a commonly used dataset in machine learning for binary classification tasks, particularly for practicing and demonstrating various machine learning algorithms and techniques. It is included as part of the scikit-learn library in Python, which is a popular machine learning library.\n",
    "\n",
    "This dataset is often referred to as the \"Breast Cancer Wisconsin (Diagnostic) dataset\" or simply the \"breast cancer dataset.\" It contains features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. The task associated with this dataset is to predict whether a breast tumor is malignant (cancerous) or benign (non-cancerous) based on these features.\n",
    "\n",
    "You can load this dataset in scikit-learn using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the Pima Indians cancer dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "# Extract features (X) and target (y) from the dataset\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "cancer_df = pd.DataFrame(data=cancer.data, columns=cancer.feature_names)\n",
    "\n",
    "# Add the target variable as a column\n",
    "cancer_df[\"target\"] = cancer.target\n",
    "\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Split the dataset into two partsl a training set and a testing (or validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into two partsl a training set and a testing (or validation) set.\n",
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "\n",
    "# Create a logistic regression model\n",
    "\n",
    "# Train the model on the training data\n",
    "\n",
    "# It will stop with no success. Click on the link to learn more about preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, the `StandardScaler` is a preprocessing technique used to standardize the features of a dataset, meaning it scales the features to have a mean of 0 and a standard deviation of 1. This is often referred to as `z-score normalization` or `standardization`. The process of standardization helps ensure that all features have the same scale and can prevent certain features from dominating the model's learning process.\n",
    "\n",
    "The `.fit()` method of the StandardScaler computes the mean and standard deviation for each feature in the dataset based on the provided data. The mean and standard deviation are used to standardize the data when the `.transform()` method is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Split the `scaled` dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Split the scaled dataset\n",
    "# Split the data into training and testing sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Train the model on the training data and print the optimal coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data and find the optimal coefficients.\n",
    "# Create a logistic regression model\n",
    "\n",
    "# Train the model on the training data\n",
    "\n",
    "# Print the model's coefficients and intercept\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Make predictions for the testing data and print the confustion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the testing data and print the confustion matrix and classification report.\n",
    "# Make predictions on the test data\n",
    "\n",
    "\n",
    "# Calculate accuracy and display results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Use 5-fold cross-validation to find $R^2$ scores and their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 5-fold cross-validation to find $R^2$ scores and their average.\n",
    "\n",
    "# Perform cross-validation and calculate the scores\n",
    "\n",
    "\n",
    "# Print the cross-validation scores and their mean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TODO`: Plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "\n",
    "\"\"\"Solutions\"\"\"\n",
    "\n",
    "# Get predicted probabilities for the positive class (class 1)\n",
    "\n",
    "\n",
    "# Compute ROC curve\n",
    "\n",
    "# Calculate the AUC (Area Under the ROC Curve)\n",
    "\n",
    "\n",
    "# Plot the ROC curve\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
