{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent \n",
    "\n",
    "**A note on this document**\n",
    "This document is known as a Jupyter notebook; it allows text and executable code to coexist in a very easy-to-read format. Blocks can contain text or executable code. For blocks containing code, press `Shift + Enter`, `Ctrl+Enter`, or click the arrow on the block to run the code. Earlier blocks of code need to be run for the later blocks of code to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lesson, we explored gradient descent as a numerical method for solving linear regression problems. Gradient descent is an iterative optimization algorithm used to minimize a function by adjusting parameters in the direction of the negative gradient of the function. For linear regression, the learning rule is given by:\n",
    "\n",
    "$$ a \\leftarrow a - \\alpha \\frac{\\partial}{\\partial a} \\mathcal{L}(\\mathbf{w}) $$\n",
    "$$ b \\leftarrow b - \\alpha \\frac{\\partial}{\\partial b} \\mathcal{L}(\\mathbf{w}) $$\n",
    "\n",
    "Here, $\\alpha$ is the learning rate, which controls the size of the steps we take towards the minimum. $\\mathcal{L}(\\mathbf{w})$ represents the cost function, which measures how well our model's predictions match the actual data. Specifically, we used the residual sum of squares (RSS) as the loss function, $\\mathcal{L}(\\mathbf{w})$. The partial derivatives of the loss function with respect to $a$ and $b$ are:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial a} \\mathcal{L}(\\mathbf{w}) &= \\sum_{n=1}^N 2 e_n \\tag{1} \\\\ \n",
    "\\frac{\\partial}{\\partial b} \\mathcal{L}(\\mathbf{w}) &= \\sum_{n=1}^N 2 e_n x_n \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "In these equations, $e_n = \\hat{t}_n - t_n$, where $\\hat{t}_n = ax_n + b$. This approach is particularly effective for a simple first-order model, expressed as $\\hat{t} = ax + b$.\n",
    "\n",
    "By iteratively updating $a$ and $b$ using these rules, we can minimize the cost function and find the best-fit line for our data. This method is fundamental in machine learning and helps in understanding more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's delve deeper by developing a more versatile function capable of handling multidimensional models, such as $\\hat{t} = \\mathbf{w}^\\top \\mathbf{x} = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_K x_K$.\n",
    "\n",
    "The learning rule for this multidimensional model is expressed as:\n",
    "\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) $$\n",
    "\n",
    "For the first-order model we discussed earlier, our vectors are defined as follows: $\\mathbf{w} =[w_0 \\quad w_1]^\\top = [b \\quad a]^\\top$ and $\\mathbf{x} =[1 \\quad x]^\\top$. Consequently, we can combine equations (1) and (2) into:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w} } \\mathcal{L}(\\mathbf{w}) = \\sum_{n=1}^N 2 e_n \\mathbf{x}_n \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "This simplification is possible because we can express:\n",
    "\\begin{equation}\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) = \\frac{\\partial}{\\partial \\mathbf{w} } \\mathcal{L}(\\mathbf{w}) = \\begin{bmatrix} \\frac{\\delta}{\\delta a} \\mathcal{L}(\\mathbf{w}) \\\\\n",
    "\\frac{\\partial}{\\partial b} \\mathcal{L}(\\mathbf{w})  \\end{bmatrix} \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "And also:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{n=1}^N e_n \\mathbf{x}_n = \\begin{bmatrix}  \\sum_{n=1}^N  e_n \\cdot 1\\\\ \n",
    " \\sum_{n=1}^N  e_n x_n \\end{bmatrix} \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "By understanding these principles, we can extend our gradient descent approach to more complex, multidimensional models, enhancing our ability to tackle a wider range of machine learning problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create a Python function like this:\n",
    "\n",
    "```python\n",
    "def gradient_descent(X, t, learning_rate, num_iterations):\n",
    "```\n",
    "\n",
    "This function takes a dataset of size $N$ with $K$ features, represented as $X\\in\\mathbb{R}^{N\\times (K+1)}$, along with their corresponding target values, represented as $t\\in\\mathbb{R}^{N}$. In other words:\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1K} \\\\ 1 & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N1} & \\cdots &x_{NK} \\end{bmatrix}$$\n",
    "and \n",
    "$$t = \\begin{bmatrix}  t_{1} & \\cdots & t_{N} \\end{bmatrix}^\\top$$\n",
    "\n",
    "Here, $X$ is the feature matrix where each row represents a sample and each column represents a feature, including a bias term (the column of ones). The vector $t$ contains the target values for each sample.\n",
    "\n",
    "We calculate the prediction for the $n$-th sample as $\\hat{t}_n = \\mathbf{w}^\\top \\mathbf{x}_n$ or $\\hat{t}_n = w_0 + w_1x_{n1} + \\cdots + w_Kx_{nK}$.\n",
    "\n",
    "Then, we can represent the predictions vector $\\hat{\\mathbf{t}}$ as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{t}} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{t}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{t}_N\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    " w_0 + w_1x_{11} + \\cdots + w_K x_{1K} \\\\ \n",
    " \\vdots  \\\\ \n",
    " w_0 + w_1x_{N1} + \\cdots + w_Kx_{NK}\n",
    "\\end{bmatrix} \n",
    "\\end{equation*}\n",
    "\n",
    "Therefore, we can express $\\hat{\\mathbf{t}}$ as:\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{t}} = \\begin{bmatrix} \n",
    "1 & x_{11} & \\cdots & x_{1K} \\\\ \n",
    "1 & \\vdots & \\ddots & \\vdots \\\\ \n",
    "1 & x_{N1} & \\cdots & x_{NK} \\end{bmatrix} \n",
    "\\begin{bmatrix} w_0 \\\\ \\vdots  \\\\ w_K \\end{bmatrix} = X\\mathbf{w}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, you can implement this as:\n",
    "\n",
    "```python\n",
    "predictions = np.dot(X, w)  # or X @ w (note it should not be w @ X)\n",
    "```\n",
    "\n",
    "After obtaining predictions, you can easily calculate errors using the equation $\\mathbf{e} = \\hat{\\mathbf{t}} - \\mathbf{t}$. In Python, you can express it as:\n",
    "\n",
    "```python\n",
    "errors = predictions - t    # errors = t_hat - t   \n",
    "```\n",
    "\n",
    "Note that:\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\begin{bmatrix} \n",
    "e_1 \\\\ \n",
    "\\vdots \\\\ \n",
    "e_N \n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "\\hat{t}_1 - t_1 \\\\ \n",
    "\\vdots \\\\ \n",
    "\\hat{t}_N - t_N \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, we need to implement the gradient in Python. The gradient for linear regression problems with $N$ measurements can be expressed as:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{w}) =  \\sum_{n=1}^N e_n \\mathbf{x}_n\n",
    "$$\n",
    "\n",
    "Note that we have dropped the coefficient 2 in front of $e_n$, which is a common practice in gradient descent because the coefficient can be incorporated into the learning rate.\n",
    "\n",
    "For linear regression problems with $K$ features, the gradient can be expressed as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{n=1}^N e_n \\mathbf{x}_n = \\begin{bmatrix}  \n",
    "\\sum_{n=1}^N  e_n \\cdot 1\\\\ \n",
    "\\sum_{n=1}^N  e_n x_{n1} \\\\ \n",
    "\\vdots \\\\\n",
    "\\sum_{n=1}^N  e_n x_{nK} \n",
    "\\end{bmatrix} = X^\\top \\mathbf{e}\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore, the gradient can be easily implemented in Python like this:\n",
    "\n",
    "```python\n",
    "gradient = np.dot(X.T, errors)\n",
    "```\n",
    "\n",
    "for $\\mathcal{L}(\\mathbf{w}) = RSS(\\mathbf{w})$ or\n",
    "\n",
    "```python\n",
    "gradient = (1 / num_samples) * np.dot(X.T, errors)\n",
    "```\n",
    "\n",
    "for $\\mathcal{L}(\\mathbf{w}) = MSE(\\mathbf{w})$.\n",
    "\n",
    "By implementing these steps, we can effectively use gradient descent to optimize our linear regression models, whether they are simple or multidimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 1\n",
    "\n",
    "Show that\n",
    "\\begin{equation*}\n",
    "\\sum_{n=1}^N e_n \\mathbf{x}_n =  X^\\top \\mathbf{e} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 2\n",
    "\n",
    "Complete the `gradient_descent` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def gradient_descent(X, t, learning_rate, num_iterations):\n",
    "    # Initialize model parameters with zeros\n",
    "    num_samples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "\n",
    "    # Perform gradient descent\n",
    "    for _ in range(num_iterations):\n",
    "        \"\"\"\n",
    "        Write your code here\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverable 3\n",
    "\n",
    "Use the following code to test your `gradient descent` function.\n",
    "\n",
    "The function to test $t = 2x_1 + 3x_2 + \\zeta$.  \n",
    "\n",
    "Here $\\mathbf{w} = [0 \\quad 2 \\quad 3]^\\top$ and $\\zeta$ is a Guassian noise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data (replace with your dataset)\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 2)\n",
    "t = 2 * X[:, 0] + 3 * X[:, 1] + 0.1 * np.random.rand(100)\n",
    "\n",
    "\"\"\" Write your code here \"\"\"\n",
    "# Add a column of ones to X for the intercept term\n",
    "# X = 0\n",
    "\n",
    "# # Select parameters\n",
    "# learning_rate = 0.1\n",
    "# num_iterations = 10\n",
    "\n",
    "\"\"\" end of your code \"\"\"\n",
    "\n",
    "# Perform gradient descent\n",
    "learned_parameters = gradient_descent(X, t, learning_rate, num_iterations)\n",
    "\n",
    "print(\"Learned Parameters (w):\", learned_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.scatter(X[:, 0], X[:, 1], t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 4\n",
    "\n",
    "Use the `Falling Body` example to test your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../data/falling_body.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data[\"time\"].to_numpy()\n",
    "y = data[\"y\"].to_numpy()\n",
    "print(t)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(t, y, \"o\")\n",
    "plt.xlabel(\"time, sec\")\n",
    "plt.ylabel(\"height (y), m\", rotation=90)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Write your code here \"\"\"\n",
    "\n",
    "# Add a column of ones to X for the intercept term\n",
    "# X = 0\n",
    "\n",
    "# print(X.shape) should return\n",
    "# (25, 3)\n",
    "\n",
    "# print(X) should return\n",
    "\"\"\"\n",
    "[[ 1.      0.      0.    ]\n",
    " [ 1.      0.25    0.0625]\n",
    " [ 1.      0.5     0.25  ]\n",
    "   :\n",
    "\"\"\"\n",
    "\n",
    "# # Select parameters\n",
    "# learning_rate = 0.1\n",
    "# num_iterations = 10\n",
    "\n",
    "\"\"\" end of your code \"\"\"\n",
    "\n",
    "\n",
    "# Perform gradient descent\n",
    "learned_parameters = gradient_descent(X, y, learning_rate, num_iterations)\n",
    "\n",
    "print(\"Learned Parameters (w):\", learned_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0, v0, g2 = learned_parameters\n",
    "\n",
    "z = y0 + v0 * t + g2 * t * t\n",
    "\n",
    "\"\"\" Write your code here \"\"\"\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(t, y, \"o\")\n",
    "plt.plot(t, z, \"r\")\n",
    "plt.xlabel(\"time, sec\")\n",
    "plt.ylabel(\"height (y), m\", rotation=90)\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
