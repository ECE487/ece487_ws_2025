{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "\n",
    "**A note on this document**\n",
    "This document is known as a Jupyter notebook; it allows text and executable code to coexist in a very easy-to-read format. Blocks can contain text or executable code. For blocks containing code, press `Shift + Enter`, `Ctrl+Enter`, or click the arrow on the block to run the code. Earlier blocks of code need to be run for the later blocks of code to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lesson, we delved into the concept of logistic regression for binary classification. The learning rule can be expressed as follows:\n",
    "$$ \\mathbf{w} \\leftarrow \\mathbf{w} -\\alpha \\nabla \\mathcal{L}(\\mathbf{w}) $$\n",
    "\n",
    "\n",
    "Here, $\\nabla \\mathcal{L}(\\mathbf{w})$ is defined as:\n",
    "$$\\nabla \\mathcal{L}(\\mathbf{w}) = \\sum_{n=1}^N e_n \\mathbf{x}_n$$\n",
    "\n",
    "In this context, $e_n$ is calculated as $t_n - \\hat{t}_n$, where the prediction $\\hat{t}_n$ is determined by:\n",
    "\n",
    "$$\\hat{t}_n = \\sigma(\\mathbf{w}^\\top \\mathbf{x}_n) = \\sigma(w_0 + w_1x_{n1} + w_2x_{n2} + \\cdots + w_Kx_{nK})$$\n",
    "\n",
    "The sigmoid function $\\sigma(\\cdot)$, defined as:\n",
    "\n",
    "$$ \\sigma(\\gamma) = \\frac{1}{1+e^{-\\gamma}}$$\n",
    "\n",
    "is used to calculate $\\hat{t}_n$. \n",
    "\n",
    "It's worth noting that in the case of linear regression, the prediction was given simply by:\n",
    "\n",
    "$$\\hat{t}_n = \\mathbf{w}^\\top \\mathbf{x}_n = w_0 + w_1x_{n1} + w_2x_{n2} + \\cdots + w_Kx_{nK}$$\n",
    "\n",
    "This comparison helps illustrate the difference between logistic regression and linear regression in terms of their prediction functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's represent the predictions vector for logistic regression, denoted as $\\hat{\\mathbf{t}}$, in a concise manner:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{t}} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{t}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{t}_N\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    " \\sigma(w_0 + w_1x_{11} + \\cdots + w_K x_{1K}) \\\\ \n",
    " \\vdots  \\\\ \n",
    " \\sigma(w_0 + w_1x_{N1} + \\cdots + w_Kx_{NK})\n",
    "\\end{bmatrix} \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "In a more compact notation, we can express $\\hat{\\mathbf{t}}$ as:\n",
    "\\begin{equation*}\n",
    "\\hat{\\mathbf{t}} = \\sigma \\left( \\begin{bmatrix} 1 & x_{11} & \\cdots & x_{1K} \\\\ 1 & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N1} & \\cdots & x_{NK} \\end{bmatrix} \\begin{bmatrix} w_0 \\\\ \\vdots  \\\\ w_K \\end{bmatrix} \\right) = \\sigma(X\\mathbf{w})\n",
    "\\end{equation*}\n",
    "\n",
    "This compact notation allows you to implement it in Python as:\n",
    "\n",
    "`predictions = sigmoid(np.dot(X, w))  # or sigmoid(X@w) `  \n",
    "\n",
    "In this expression, `X` represents the feature matrix with dimensions $N\\times(K+1)$, `w` is the weight vector of size $(K+1)$, and `sigmoid` is the _sigmoid_ function applied element-wise to the result of the dot product. This calculation efficiently computes the predictions for logistic regression in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a sigmoid function in Python and perform a test. This function will be essential for implementing logistic regression later in this lab.\n",
    "\n",
    "## Deliverable 1\n",
    "\n",
    "Complete the `sigmoid` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# sigmoid (logistic) function\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Write your code here for Lab3\n",
    "    \"\"\"\n",
    "    return 0\n",
    "\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "sig = sigmoid(x)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "plt.plot(x, sig, \"b\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sigmoid\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can employ a 2-dimensional sigmoid function for two-dimensional data, like when dealing with $t = w_1x_1 + w_2x_2$. To gain a better understanding of this 2D sigmoid function, we can create a surface plot for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# Create a grid of points in the x and y dimensions\n",
    "x = np.linspace(-10, 10, 100)  # Adjust the range and granularity as needed\n",
    "y = np.linspace(-10, 10, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Compute the sigmoid values for the grid\n",
    "Z = sigmoid(X + Y)\n",
    "\n",
    "# Create a 3D surface plot\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.plot_surface(X, Y, Z, cmap=\"viridis\")\n",
    "\n",
    "# Add labels and a title\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Sigmoid Output\")\n",
    "ax.set_title(\"Two-Dimensional Sigmoid Function (Surface Plot)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of a dataset that we can use to test logistic regression. This dataset represents a binary classification problem where we want to predict whether a student will pass (1) or fail (0) an exam based on the number of hours they studied and the number of hours they slept üòÅ:\n",
    "\n",
    "We can use this dataset to perform binary classification using logistic regression. The goal is to build a model that predicts whether a student will pass the exam based on the hours they studied and slept. We can split this dataset into a training set and a testing set to evaluate your logistic regression model's performance, but we will not evaluate the performance in this lab. \n",
    "\n",
    "Please read the code below carefully. When it comes to your final project, you will be creating everything from the ground up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_exam_data():\n",
    "    # Specify the file name\n",
    "    file_name = \"./data/exam_data.csv\"\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "\n",
    "exam_df = read_exam_data()\n",
    "print(exam_df)\n",
    "\n",
    "# Separate the data into Passed (Y=1) and Failed (Y=0)\n",
    "passed = exam_df[exam_df[\"Passed Exam (Y)\"] == 1]\n",
    "failed = exam_df[exam_df[\"Passed Exam (Y)\"] == 0]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(\n",
    "    passed[\"Hours Studied (X1)\"],\n",
    "    passed[\"Hours Slept (X2)\"],\n",
    "    label=\"Passed\",\n",
    "    color=\"r\",\n",
    "    marker=\"o\",\n",
    "    facecolors=\"none\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    failed[\"Hours Studied (X1)\"],\n",
    "    failed[\"Hours Slept (X2)\"],\n",
    "    label=\"Failed\",\n",
    "    color=\"b\",\n",
    "    marker=\"o\",\n",
    "    facecolors=\"none\",\n",
    ")\n",
    "plt.xlabel(\"Hours Studied (X1)\")\n",
    "plt.ylabel(\"Hours Slept (X2)\")\n",
    "plt.title(\"Exam Data Scatter Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(\n",
    "    exam_df[\"Hours Studied (X1)\"],\n",
    "    exam_df[\"Passed Exam (Y)\"],\n",
    "    color=\"b\",\n",
    "    marker=\"o\",\n",
    "    facecolors=\"none\",\n",
    ")\n",
    "plt.xlabel(\"Hours Studied (X1)\")\n",
    "plt.ylabel(\"Passed Exam (Y)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(\n",
    "    exam_df[\"Hours Slept (X2)\"],\n",
    "    exam_df[\"Passed Exam (Y)\"],\n",
    "    color=\"r\",\n",
    "    marker=\"o\",\n",
    "    facecolors=\"none\",\n",
    ")\n",
    "plt.xlabel(\"Hours Slept (X2)\")\n",
    "plt.ylabel(\"Passed Exam (Y)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable 2\n",
    "\n",
    "Implement the `gradient_descent` function provided below. This gradient descent function is intended for logistic regression and closely resembles the one used for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic loss (cross-entropy) function\n",
    "def logistic_loss(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    loss = -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define gradient descent function\n",
    "def gradient_descent(X, t, learning_rate, num_iterations):\n",
    "    # Initialize model parameters with zeros\n",
    "    num_samples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "    losses = []\n",
    "\n",
    "    # TODO: Perform gradient descent\n",
    "    for _ in range(num_iterations):\n",
    "        \"\"\"\n",
    "        Write your code here\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        print(w)\n",
    "\n",
    "    return w, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = exam_df[\"Hours Studied (X1)\"]\n",
    "x2 = exam_df[\"Hours Slept (X2)\"]\n",
    "t = exam_df[\"Passed Exam (Y)\"]\n",
    "\n",
    "X = np.column_stack([np.ones_like(x1), x1, x2])\n",
    "print(X.shape)\n",
    "print(t.shape)\n",
    "learning_rate = 0.65\n",
    "num_iterations = 50000\n",
    "\n",
    "w, losses = gradient_descent(X, t, learning_rate, num_iterations)\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to evaluate your parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.where(X @ w > 0.5, 1, 0)\n",
    "print(np.column_stack([t, predictions]))\n",
    "match = np.count_nonzero(t == predictions)\n",
    "\n",
    "# The count of match values shoud be no less than 65\n",
    "print(f\"Count of match is {match} out {len(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: Update the following line with the optimal parameters (weights)\n",
    "w = np.array([0,0,0])\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(\n",
    "    passed[\"Hours Studied (X1)\"],\n",
    "    passed[\"Hours Slept (X2)\"],\n",
    "    label=\"Passed\",\n",
    "    color=\"r\",\n",
    "    marker=\"o\",\n",
    "    facecolors=\"none\",\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    failed[\"Hours Studied (X1)\"],\n",
    "    failed[\"Hours Slept (X2)\"],\n",
    "    label=\"Failed\",\n",
    "    color=\"b\",\n",
    "    marker=\"o\",\n",
    "    facecolors=\"none\",\n",
    ")\n",
    "plt.xlabel(\"Hours Studied (X1)\")\n",
    "plt.ylabel(\"Hours Slept (X2)\")\n",
    "\n",
    "# Decision boundary\n",
    "x1_vals = np.linspace(min(x1), 4, 100)\n",
    "x2_vals = -(w[0] + w[1] * x1_vals) / w[2]\n",
    "plt.plot(x1_vals, x2_vals, color=\"green\", label=\"Decision Boundary\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title(\"Logistic Regression Decision Boundary\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
